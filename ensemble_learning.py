# -*- coding: utf-8 -*-
"""ensemble-learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cfXCHKyObXFFU-8SpCZqwLl4We_K9j4k
"""

# Set directories
base_dir = '/content/drive/MyDrive/duzenlenmis'
train_dir = os.path.join(base_dir, 'train')
test_dir = os.path.join(base_dir, 'test')

from keras.preprocessing.image import ImageDataGenerator

# Create ImageDataGenerators
train_datagen = ImageDataGenerator(rescale=1./255,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical'
)

test_datagen = ImageDataGenerator(rescale=1./255)

test_generator_100 = test_datagen.flow_from_directory(
        test_data_dir,
        target_size=(img_height, img_width),
        batch_size=100,  # Set batch size to 100
        class_mode='categorical',
        shuffle=False)  # Keep data in original order

# Now this generator only includes first 100 images.

from tensorflow.keras.applications import ResNet50, ResNet101

def create_model(base_model):
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(1024, activation='relu')(x)
    predictions = Dense(num_classes, activation='softmax')(x)
    model = Model(inputs=base_model.input, outputs=predictions)
    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Define the models
base_model1 = DenseNet169(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))
base_model2 = ResNet50(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))
base_model3 = ResNet101(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))

model1 = create_model(base_model1)
model2 = create_model(base_model2)
model3 = create_model(base_model3)

# Train the models
model1.fit(train_generator, steps_per_epoch=train_generator.n // train_generator.batch_size, epochs=14)
model2.fit(train_generator, steps_per_epoch=train_generator.n // train_generator.batch_size, epochs=14)
model3.fit(train_generator, steps_per_epoch=train_generator.n // train_generator.batch_size, epochs=14)

def ensemble_predict(models, weights, generator):
    outputs = [model.predict(generator) for model in models]
    output = np.zeros_like(outputs[0])
    for weight, output_i in zip(weights, outputs):
        output += weight * output_i
    return np.argmax(output, axis=-1)

# Ensemble prediction
weights = [0.5, 0.3, 0.2]
models = [model1, model2, model3]

test_predictions_100 = ensemble_predict(models, weights, test_generator_100)

from sklearn.metrics import accuracy_score

# Get the true labels
true_labels = test_generator_100.classes[:100]  # Only take the first 100 labels

# Compute the accuracy
accuracy = accuracy_score(true_labels, test_predictions_100)

print("Ensemble accuracy: ", accuracy)