# -*- coding: utf-8 -*-
"""hyperopt-parameters.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12_JM_ulqPZOJ_K2pNaSaohAo35-FRQTi
"""

import os
import pandas as pd
import numpy as np
import shutil
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.applications import DenseNet169
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dropout
from tensorflow.keras.optimizers import RMSprop, SGD
import random
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials
from google.colab import drive
drive.mount('/content/drive')

# Set directories
base_dir = '/content/drive/MyDrive/Colab Notebooks/alzheimer/alzheimer'
train_dir = os.path.join(base_dir, 'train')
train_dir_copy = os.path.join(base_dir, 'train_copy')
test_dir = os.path.join(base_dir, 'test')

# Make a copy of the train directory
if os.path.exists(train_dir_copy):
    shutil.rmtree(train_dir_copy)
shutil.copytree(train_dir, train_dir_copy)

# Create a DataFrame to keep track of augmentation and deletion
df = pd.DataFrame(columns=['Image', 'Class', 'AugCount', 'Deleted'])

num_classes = 4  # Number of grades

# Define the search space for hyperparameters
space = {
    'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(0.001)),
    'num_dense_layers': hp.choice('num_dense_layers', [1, 2, 3]),
    'dropout_rate': hp.choice('dropout_rate', [0.2, 0.3, 0.4]),
    'optimizer': hp.choice('optimizer', ['adam', 'rmsprop']),
    'units': hp.choice('units', [512, 1024, 2048]),
    'shear_range': hp.uniform('shear_range', 0.1, 0.3),
    'zoom_range': hp.uniform('zoom_range', 0.1, 0.3),
    'epochs': hp.choice('epochs', range(5, 21))
}

def train_model(params):
    learning_rate = params['learning_rate']
    num_dense_layers = params['num_dense_layers']
    dropout_rate = params['dropout_rate']
    optimizer = params['optimizer']
    units = params['units']
    shear_range = params['shear_range']
    zoom_range = params['zoom_range']
    epochs = params['epochs']

    print(f"\nStarting training with parameters: \nLearning rate: {learning_rate}, Dense layers: {num_dense_layers}, Dropout rate: {dropout_rate}, Optimizer: {optimizer}, Units: {units}, Shear range: {shear_range}, Zoom range: {zoom_range}, Epochs: {epochs}\n")

    img_width, img_height = 224, 224  # default for DenseNet
    batch_size = 32

    # Create ImageDataGenerators
    train_datagen = ImageDataGenerator(rescale=1./255,
                                       shear_range=shear_range,
                                       zoom_range=zoom_range,
                                       horizontal_flip=True)

    test_datagen = ImageDataGenerator(rescale=1./255)

    train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=(img_width, img_height),
        batch_size=batch_size,
        class_mode='categorical'
    )

    test_generator = test_datagen.flow_from_directory(
        test_dir,
        target_size=(img_width, img_height),
        batch_size=batch_size,
        class_mode='categorical',
        shuffle=False  # Important for correct label correspondence
    )

    # Load the base model
    base_model = DenseNet169(weights='imagenet', include_top=False,
                             input_shape=(img_width, img_height, 3))

    # Add a global spatial average pooling layer
    x = base_model.output
    x = GlobalAveragePooling2D()(x)

    # Add a fully-connected layer and a logistic layer with num_classes nodes (one for each class)
    for _ in range(num_dense_layers):
        x = Dense(units, activation='relu')(x)
        x = Dropout(dropout_rate)(x)
    predictions = Dense(num_classes, activation='softmax')(x)

    # This is the model we will train
    model = Model(inputs=base_model.input, outputs=predictions)

    # compile the model
    if optimizer == 'adam':
        opt = Adam(learning_rate=learning_rate)
    elif optimizer == 'rmsprop':
        opt = RMSprop(learning_rate=learning_rate)
    else:
        opt = SGD(learning_rate=learning_rate)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])


    model.fit(train_generator,
              steps_per_epoch=train_generator.n // train_generator.batch_size,
              epochs=epochs
              )

    # Make predictions on the test set
    test_predictions = model.predict(test_generator)
    pred_classes = np.argmax(test_predictions, axis=-1)
    test_labels = test_generator.labels

    # Calculate accuracy
    accuracy = np.mean(pred_classes == test_labels)

    # return loss as the thing to be minimized
    loss = 1 - accuracy
    return {'loss': loss, 'status': STATUS_OK, 'model': model}


trials = Trials()
best = fmin(fn=train_model, space=space, algo=tpe.suggest, max_evals=25, trials=trials)

print('Best parameters:', best)

